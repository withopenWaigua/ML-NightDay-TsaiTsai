{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/withopenWaigua/ML-NightDay-TsaiTsai/blob/main/Lesson_11_AdaBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGEjPIYWXyYN"
      },
      "source": [
        "# 集成学习"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFbAMYUfXyYP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib as mlp\n",
        "import seaborn as sns\n",
        "import re, pip, conda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5nZYWg9XyYR",
        "outputId": "76f3e970-d9c6-4ed7-ec18-0f3b345c6934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sklearn 1.0.1\n",
            "matplotlib 3.4.3\n",
            "numpy 1.21.4\n",
            "pandas 1.3.4\n",
            "seaborn 0.11.2\n",
            "pip 21.3.1\n",
            "conda 4.11.0\n"
          ]
        }
      ],
      "source": [
        "for package in [sklearn,mlp,np,pd,sns,pip,conda]:\n",
        "    print(re.findall(\"([^']*)\",str(package))[2],package.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbrfu_fAXyYS"
      },
      "outputs": [],
      "source": [
        "#pip install --upgrade scikit-learn\n",
        "#conda update scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qPB3pzuXyYS"
      },
      "source": [
        "目录\n",
        "\n",
        "一 Boosting方法的基本思想<br>\n",
        "&emsp;1 Bagging pk Boosting<br>\n",
        "&emsp;2 Boosting算法的基本元素与基本流程<br>\n",
        "&emsp;3 sklearn中的boosting算法<br>\n",
        "二 AdaBoost<br>\n",
        "&emsp;1 AdaBoost的基本参数与损失函数<br>\n",
        "&emsp;&emsp;1.1 n_estimators<br>\n",
        "&emsp;&emsp;1.2 learning_rate<br>\n",
        "&emsp;&emsp;1.3 algorithm与loss<br>\n",
        "&emsp;2 原理进阶：Adaboost回归的求解流程<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XD2b1DQXyYT"
      },
      "source": [
        "# 一 Boosting方法的基本思想"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKE2QNhLXyYT"
      },
      "source": [
        "在集成学习的“弱分类器集成”领域，除了降低方差来降低整体泛化误差的装袋法Bagging，还有专注于**降低整体偏差**来降低泛化误差的提升法Boosting。相比起操作简单、大道至简的Bagging算法，Boosting算法在操作和原理上的难度都更大，但由于专注于偏差降低，Boosting算法们在模型效果方面的突出表现制霸整个弱分类器集成的领域。当代知名的Boosting算法当中，Xgboost，LightGBM与Catboost都是机器学习领域最强大的强学习器，Boosting毫无疑问是当代机器学习领域最具统治力的算法领域。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBoyLUXcXyYU"
      },
      "source": [
        "- **Boosting PK Bagging**\n",
        "\n",
        "||装袋法 Bagging|提升法 Boosting|\n",
        "|-|-|-|\n",
        "|弱评估器|**相互独立**，并行构建|**相互关联**，按顺序依次构建<br>先建弱分类器的预测效果影响后续模型的建立|\n",
        "|建树前的抽样方式|样本有放回抽样<br>特征无放回抽样|样本有放回抽样<br>特征无放回抽样<br>先建弱分类器的预测效果可能影响抽样细节|\n",
        "|集成的结果|回归平均<br>分类众数|每个算法**具有自己独特的规则**，一般来说：<br>(1) 表现为某种分数的加权平均<br>(2) 使用输出函数|\n",
        "|目标|**降低方差**<br>提高模型整体的稳定性来提升泛化能力<br>本质是从“平均”这一数学行为中获利|**降低偏差**<br>提高模型整体的精确度来提升泛化能力<br>相信众多弱分类器叠加后可以等同于强学习器|\n",
        "|单个评估器容易<br>过拟合的时候|具有一定的抗过拟合能力|具有一定的抗过拟合能力|\n",
        "|单个评估器的效力<br>比较弱的时候|可能失效|大概率会提升模型表现|\n",
        "|代表算法|随机森林|梯度提升树，Adaboost|\n",
        "\n",
        "![RF2](https://pictes.oss-cn-beijing.aliyuncs.com/%E5%BE%AE%E8%AF%BE%20-%20sklearn/RFC/RF2.png)\n",
        "\n",
        "在以随机森林为代表的Bagging算法中，我们一次性建立多个平行独立的弱评估器，并让所有评估器并行运算。在Boosting集成算法当中，我们逐一建立多个弱评估器（基本是决策树），并且下一个弱评估器的建立方式依赖于上一个弱评估器的评估结果，最终综合多个弱评估器的结果进行输出，因此Boosting算法中的弱评估器之间不仅不是相互独立的、反而是强相关的，同时Boosting算法也不依赖于弱分类器之间的独立性来提升结果，这是Boosting与Bagging的一大差别。如果说Bagging不同算法之间的核心区别在于靠以不同方式实现“独立性”（随机性），**那Boosting的不同算法之间的核心区别就在于上一个弱评估器的评估结果具体如何影响下一个弱评估器的建立过程**。\n",
        "\n",
        "与Bagging算法中统一的回归求平均、分类少数服从多数的输出不同，Boosting算法在结果输出方面表现得十分多样。早期的Boosting算法的输出一般是最后一个弱评估器的输出，当代Boosting算法的输出都会考虑整个集成模型中全部的弱评估器。**一般来说，每个Boosting算法会其以独特的规则自定义集成输出的具体形式**，但对大部分算法而言，集成算法的输出结果往往是**关于弱评估器的某种结果的加权平均**，其中权重的求解是boosting领域中非常关键的步骤。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwEZQu_xXyYV"
      },
      "source": [
        "- **Boosting算法的基本元素与基本流程**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6_T4C1XXyYW"
      },
      "source": [
        "基于上面所明确的“降低偏差”、“逐一建树”、以及“以独特规则输出结果”的三大特色，我们可以确立任意boosting算法的三大基本元素以及boosting算法自适应建模的基本流程：\n",
        "\n",
        "- 损失函数$L(x,y)$ ：用以衡量模型预测结果与真实结果的差异\n",
        "- 弱评估器$f(x)$ ：（一般为）决策树，不同的boosting算法使用不同的建树过程\n",
        "- 综合集成结果$H(x)$：即集成算法具体如何输出集成结果\n",
        "\n",
        "这三大元素将会贯穿所有我们即将学习的boosting算法，我们会发现几乎所有boosting算法的原理都围绕这三大元素构建。在此三大要素基础上，所有boosting算法都遵循以下流程进行建模：\n",
        "\n",
        "---\n",
        "**<font color=\"green\"><center>依据上一个弱评估器$f(x)_{t-1}$的结果，计算损失函数$L(x,y)$，\n",
        "    <br>并使用$L(x,y)$自适应地影响下一个弱评估器$f(x)_t$的构建。<br>集成模型输出的结果，受到整体所有弱评估器$f(x)_0$ ~ $f(x)_T$的影响。</center></font>**\n",
        "---\n",
        "\n",
        "正如之前所言，Boosting算法之间的不同之处就在于使用不同的方式来**影响**后续评估器的构建。无论boosting算法表现出复杂或简单的流程，其核心思想一定是围绕上面这个流程不变的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aelch0YEXyYW"
      },
      "source": [
        "- **sklearn中的boosting算法**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0CJU07FXyYX"
      },
      "source": [
        "在sklearn当中，我们可以接触到数个Boosting集成算法，包括Boosting入门算法**AdaBoost**，性能最稳定、奠定了整个Boosting效果基础的梯度提升树**GBDT**（Gradient Boosting Decision Tree），以及近几年才逐渐被验证有效的**直方提升树**（Hist Gradient Boosting Tree）。\n",
        "\n",
        "在过去5年之间，除了sklearn，研究者们还创造了大量基于GBDT进行改造的提升类算法，这些算法大多需要从第三方库进行调用，例如极限提升树**XGBoost**（Extreme Gradient Boosting Tree），轻量梯度提升树**LightGBM**（Light Gradiant Boosting Machine），以及离散提升树**CatBoost**（Categorial Boosting Tree）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4wA0rqXyYX"
      },
      "source": [
        "|Boosting算法|库|集成类|\n",
        "|:--:|:--:|:--:|\n",
        "|ADB分类|sklearn|AdaBoostClassifer|\n",
        "|ADB回归|sklearn|AdaBoostRegressor|\n",
        "|梯度提升树分类|sklearn|GradientBoostingClassifier|\n",
        "|梯度提升树回归|sklearn|GradientBoostingRegressor|\n",
        "|直方提升树分类|sklearn|HistGraidientBoostingClassifier|\n",
        "|直方提升树回归|sklearn|HistGraidientBoostingRegressor|\n",
        "|极限提升树|第三方库xgboost|xgboost.train()|\n",
        "|轻量梯度提升树|第三方库lightgbm|lightgbm.train()|\n",
        "|离散提升树|第三方库catboost|catboost.train()|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E1kk_E8XyYX"
      },
      "source": [
        "在课程当中，我们会一一介绍以上所有算法的原理与用法。另外需要注意的是，周志华老师于2017年提出的深度森林算法既不是boosting也不是bagging，而是以深度学习的思路重新集成决策树之后得到的独特算法，可以算是模型融合的一部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZm_na4fXyYY"
      },
      "source": [
        "# 二 AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zonCCpWHXyYY"
      },
      "source": [
        "AdaBoost（Adaptive Boosting，自适应提升法）是当代boosting领域的开山鼻祖，它虽然不是首个实践boosting思想算法，却是首个成功将boosting思想发扬光大的算法。它的主要贡献在于实现了两个变化：\n",
        "\n",
        "1、首次实现根据之前弱评估器的结果**自适应地**影响后续建模过程<br>\n",
        "2、在Boosting算法中，首次实现考虑全部弱评估器结果的输出方式<br>\n",
        "\n",
        "作为开山算法，AdaBoost的构筑过程非常简单：**首先，在全样本上建立一棵决策树，根据该决策树预测的结果和损失函数值，增加被预测错误的样本在数据集中的样本权重，并让加权后的数据集被用于训练下一棵决策树**。这个过程相当于有意地加重“难以被分类正确的样本”的权重，同时降低“容易被分类正确的样本”的权重，而将后续要建立的弱评估器的注意力引导到难以被分类正确的样本上。\n",
        "\n",
        "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021MachineLearning/Ensembles/Public/boostrap-fixed2.png)\n",
        "\n",
        "在该过程中，上一棵决策树的的结果通过影响样本权重、即影响数据分布来影响下一棵决策树的建立，整个过程是自适应的。当全部弱评估器都被建立后，集成算法的输出$H(x)$等于所有弱评估器输出值的加权平均，加权所用的权重也是在建树过程中被自适应地计算出来的。\n",
        "\n",
        "需要注意的是，虽然最初的原理较为简单，但近年来AdaBoost在已经发展出多个升级的版本（比如，**在建立每棵树之前，允许随机抽样特征，这使得Boosting中的决策树行为更加接近Bagging中的决策树**），而sklearn中使用了这些升级后的版本进行实现。幸运的是，这些实现并不影响我们对sklearn中的类的使用，对这些实现的具体过程感兴趣的小伙伴，可以在章节《2 原理进阶：AdaBoost的求解流程》中查看具体原理。\n",
        "\n",
        "在sklearn中，AdaBoost既可以实现分类也可以实现回归，我们使用如下两个类来调用它们：\n",
        "\n",
        "*class* `sklearn.ensemble.AdaBoostClassifier`(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
        "\n",
        "*class* `sklearn.ensemble.AdaBoostRegressor`(base_estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
        "\n",
        "不难发现，AdaBoost的参数非常非常少，在调用AdaBoost时我们甚至无需理解AdaBoost的具体求解过程。同时，ADB分类器与ADB回归器的参数也高度一致。在课程当中，我们将重点Boosting算法独有的参数，以及ADB分类与ADB回归中表现不一致的参数。\n",
        "\n",
        "|参数|参数含义|\n",
        "|:-:|:-:|\n",
        "|**base_estimator**|弱评估器|\n",
        "|n_estimators|集成算法中弱评估器的数量|\n",
        "|**learning_rate**|迭代中所使用的学习率|\n",
        "|**algorithm**（分类器专属）|用于指定分类ADB中使用的具体实现方法|\n",
        "|**loss**（回归器专属）|用于指定回归ADB中使用的损失函数|\n",
        "|random_state|用于控制每次建树之前随机抽样过程的随机数种子|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHx0tvWMXyYY"
      },
      "source": [
        "## 1 AdaBoost的基本参数与损失函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM7S_GMtXyYZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
        "from sklearn.ensemble import AdaBoostRegressor as ABR\n",
        "from sklearn.tree import DecisionTreeClassifier as DTC\n",
        "from sklearn.tree import DecisionTreeRegressor as DTR\n",
        "from sklearn.datasets import load_digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UhCCk0ZXyYZ"
      },
      "outputs": [],
      "source": [
        "#用于分类的数据\n",
        "data_c = load_digits()\n",
        "X_c = data_c.data\n",
        "y_c = data_c.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAUyucrKXyYZ",
        "outputId": "9f65f4fb-8f71-4bcd-8aba-66267581bc27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_c.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ2Nx4l5XyYa",
        "outputId": "c82fbbab-63b2-4787-a2ac-b0436025d2e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
              "       ...,\n",
              "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
              "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
              "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "actvumgHXyYa",
        "outputId": "6da0f59a-e891-4cad-bfa6-804cc81b61f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_c) #手写数字数据集，10分类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k66UC1SpXyYa"
      },
      "outputs": [],
      "source": [
        "#用于回归的数据\n",
        "data_r = pd.read_csv(r\"D:\\Pythonwork\\2021ML\\PART 2 Ensembles\\datasets\\House Price\\train_encode.csv\",index_col=0)\n",
        "X_g = data_r.iloc[:,:-1]\n",
        "y_g = data_r.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqAbkG84XyYb",
        "outputId": "527b5953-86a8-4c77-8fed-e9fef581bc23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1460, 80)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_g.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFxS_GPoXyYb",
        "outputId": "b1113211-3b16-41ed-ef37-88d5b3f867ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>住宅类型</th>\n",
              "      <th>住宅区域</th>\n",
              "      <th>街道接触面积(英尺)</th>\n",
              "      <th>住宅面积</th>\n",
              "      <th>街道路面状况</th>\n",
              "      <th>巷子路面状况</th>\n",
              "      <th>住宅形状(大概)</th>\n",
              "      <th>住宅现状</th>\n",
              "      <th>水电气</th>\n",
              "      <th>...</th>\n",
              "      <th>半开放式门廊面积</th>\n",
              "      <th>泳池面积</th>\n",
              "      <th>泳池质量</th>\n",
              "      <th>篱笆质量</th>\n",
              "      <th>其他配置</th>\n",
              "      <th>其他配置的价值</th>\n",
              "      <th>销售月份</th>\n",
              "      <th>销售年份</th>\n",
              "      <th>销售类型</th>\n",
              "      <th>销售状态</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>327.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>498.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>702.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>489.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>925.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 80 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Id  住宅类型  住宅区域  街道接触面积(英尺)   住宅面积  街道路面状况  巷子路面状况  住宅形状(大概)  住宅现状  水电气  \\\n",
              "0  0.0   5.0   3.0        36.0  327.0     1.0     0.0       3.0   3.0  0.0   \n",
              "1  1.0   0.0   3.0        51.0  498.0     1.0     0.0       3.0   3.0  0.0   \n",
              "2  2.0   5.0   3.0        39.0  702.0     1.0     0.0       0.0   3.0  0.0   \n",
              "3  3.0   6.0   3.0        31.0  489.0     1.0     0.0       0.0   3.0  0.0   \n",
              "4  4.0   5.0   3.0        55.0  925.0     1.0     0.0       0.0   3.0  0.0   \n",
              "\n",
              "   ...  半开放式门廊面积  泳池面积  泳池质量  篱笆质量  其他配置  其他配置的价值  销售月份  销售年份  销售类型  销售状态  \n",
              "0  ...       0.0   0.0   0.0   0.0   0.0      0.0   1.0   2.0   8.0   4.0  \n",
              "1  ...       0.0   0.0   0.0   0.0   0.0      0.0   4.0   1.0   8.0   4.0  \n",
              "2  ...       0.0   0.0   0.0   0.0   0.0      0.0   8.0   2.0   8.0   4.0  \n",
              "3  ...       0.0   0.0   0.0   0.0   0.0      0.0   1.0   0.0   8.0   0.0  \n",
              "4  ...       0.0   0.0   0.0   0.0   0.0      0.0  11.0   2.0   8.0   4.0  \n",
              "\n",
              "[5 rows x 80 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_g.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOIB1BvYXyYb"
      },
      "source": [
        "- 参数`base_estimator`，属性`base_estimator_`与`estimators_`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ct_v76XyYb"
      },
      "source": [
        "`base_estimator`是规定AdaBoost中使用弱评估器的参数。与对弱评估器有严格要求的Bagging算法不同，boosting算法通过降低偏差来降低整体泛化误差，因此可以使用任意弱评估器，且这些弱评估器往往被假设成非常弱小的评估器。当然了，默认的弱评估器还是决策树。在sklearn中，**ADB分类器的默认弱评估器是最大深度为1的“树桩”，ADB回归器的默认评估器是最大深度为3的“树苗”**，弱评估器本身基本不具备判断能力。而回归器中树深更深是因为boosting算法中回归任务往往更加复杂。在传统ADB理论当中，一般认为AdaBoost中的弱分类器为最大深度为1的树桩，但现在我们也可以自定义某种弱评估器来进行输入。\n",
        "\n",
        "当模型建好之后，我们可以使用属性`base_estimator_`来查看当前弱评估器，同时也可以使用`estimators_`来查看当前集成模型中所有弱评估器的情况："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv4Wegn4XyYc"
      },
      "source": [
        "- 建立集成算法，调用其中的弱评估器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgz74yoEXyYc"
      },
      "outputs": [],
      "source": [
        "#建立ADB回归器和分类器\n",
        "clf = ABC(n_estimators=3).fit(X_c,y_c)\n",
        "reg = ABR(n_estimators=3).fit(X_g,y_g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoaFkRy_XyYc",
        "outputId": "ce6cd9c9-d79e-4209-8126-b9b2ee1e7090"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=1)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.base_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k9d1IngXyYc",
        "outputId": "f5da2b36-7bd1-462d-a033-6b72989e2745"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=3)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg.base_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaG0yqu1XyYc",
        "outputId": "bec2a690-619f-4c44-93df-aaf189a1d019"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DecisionTreeRegressor(max_depth=3, random_state=765348147),\n",
              " DecisionTreeRegressor(max_depth=3, random_state=850911835),\n",
              " DecisionTreeRegressor(max_depth=3, random_state=1434155639)]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg.estimators_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acjPM5OPXyYd"
      },
      "source": [
        "当AdaBoost完成分类任务时，弱评估器是分类树，当AdaBoost完成回归任务时，弱评估器是回归树，这一点与之后的Boosting算法们有较大的区别。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6iKu1eoXyYd"
      },
      "source": [
        "- 自建弱评估器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6mh2zeZXyYd"
      },
      "outputs": [],
      "source": [
        "base_estimator = DTC(max_depth=10,max_features=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXd7BDiUXyYd"
      },
      "outputs": [],
      "source": [
        "clf = ABC(base_estimator = base_estimator, n_estimators=3).fit(X_c,y_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB3bGtR5XyYd",
        "outputId": "567f9754-821d-499a-8b31-8dbd7590e25f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=10, max_features=30)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.base_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEF-bQhzXyYe",
        "outputId": "6cec91a1-6f8a-4216-950e-aca0ef285907"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DecisionTreeClassifier(max_depth=10, max_features=30, random_state=814836020),\n",
              " DecisionTreeClassifier(max_depth=10, max_features=30, random_state=880262373),\n",
              " DecisionTreeClassifier(max_depth=10, max_features=30, random_state=925249775)]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.estimators_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skqDcZVLXyYe"
      },
      "source": [
        "注意，为了保证集成算法中的树不一致，AdaBoost会默认消除我们填写在弱评估器中的random_state："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpvEuRY2XyYe"
      },
      "outputs": [],
      "source": [
        "base_estimator = DTC(max_depth=10,max_features=30,random_state=1412)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ADn9bpbXyYe"
      },
      "outputs": [],
      "source": [
        "clf = ABC(base_estimator = base_estimator, n_estimators=3).fit(X_c,y_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thDYoNUfXyYe",
        "outputId": "42936a89-233b-46cb-8468-94a2597e81dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DecisionTreeClassifier(max_depth=10, max_features=30, random_state=677195652),\n",
              " DecisionTreeClassifier(max_depth=10, max_features=30, random_state=1650391099),\n",
              " DecisionTreeClassifier(max_depth=10, max_features=30, random_state=672741048)]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.estimators_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCYKjfCRXyYf"
      },
      "source": [
        "- 参数`learning_rate`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TCt4JsvXyYf"
      },
      "source": [
        "在Boosting集成方法中，集成算法的输出$H(x)$往往都是多个弱评估器的输出结果的加权平均结果。但$H(x)$并不是在所有树建好之后才统一加权求解的，而是在算法逐渐建树的过程当中就随着迭代不断计算出来的。例如，对于样本$x_i$，集成算法当中一共有$T$棵树（也就是参数`n_estimators`的取值），现在正在建立第$t$个弱评估器，则第$t$个弱评估器上$x_i$的结果可以表示为$f_t(x_i)$。假设整个Boosting算法对样本$x_i$输出的结果为$H(x_i)$，则该结果一般可以被表示为t=1~t=T过程当中，所有弱评估器结果的加权求和："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci1Ho7ljXyYf"
      },
      "source": [
        "$$H(x_i) =  \\sum_{t=1}^T\\phi_tf_t(x_i)$$\n",
        "\n",
        "其中，$\\phi_t$为第t棵树的权重。对于第$t$次迭代来说，则有：\n",
        "\n",
        "$$H_t(x_i) = H_{t-1}(x_i) + \\phi_tf_t(x_i)$$\n",
        "\n",
        "在这个一般过程中，每次将本轮建好的决策树加入之前的建树结果时，可以在权重$\\phi$前面增加参数$\\color{red}\\eta$，表示为第t棵树加入整体集成算法时的学习率，对标参数`learning_rate`。\n",
        "\n",
        "$$H_t(x_i) = H_{t-1}(x_i) + \\boldsymbol{\\color{red}\\eta} \\phi_tf_t(x_i)$$\n",
        "\n",
        "该学习率参数控制Boosting集成过程中$H(x_i)$的增长速度，是相当关键的参数。当学习率很大时，$H(x_i)$增长得更快，我们所需的n_estimators更少，当学习率较小时，$H(x_i)$增长较慢，我们所需的n_estimators就更多，因此boosting算法往往会需要在n_estimators与learning_rate当中做出权衡（以XGBoost算法为例）。\n",
        "\n",
        "![](https://pictes.oss-cn-beijing.aliyuncs.com/%E5%BE%AE%E8%AF%BE%20-%20sklearn/week%2011%20XGBoost/eta.PNG)\n",
        "\n",
        "需要注意的是，**以上式子为boosting算法中计算方式的一般规则，并不是具体到AdaBoost或任意Boosting集成算法的具体公式**。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw77Y-LIXyYf"
      },
      "source": [
        "- 参数`algorithm`与`loss`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEXMMg26XyYf"
      },
      "source": [
        "参数`algorithm`与`loss`是boosting算法中非常常见的，分类器与回归器展示出不同参数的情况。正如之前提到的，虽然AdaBoost算法的原理简单，但是在近几年已经发展出了多种不同的算法实践手段，而参数`algorithm`与`loss`正是用来控制算法实践手段的关键参数，其中`algorithm`控制具体的实践算法，`loss`控制该实践算法中所使用的具体损失函数。\n",
        "\n",
        "> - `algorithm`\n",
        "\n",
        "首先，参数`algorithm`是针对分类器设置的参数，其中备选项有\"SAMME\"与\"SAMME.R\"两个字符串。这两个字符串分别代表了两种不同的、实现AdaBoost分类的手段：AdaBoost-SAMME与AdaBoost-SAMME.R。两者在数学流程上的区别并不大，只不过SAMME是基于算法输出的具体分类结果（例如-1，1，2）进行计算，而SAMME.R则是在SAMME基础上改进过后、基于弱分配器输出的概率值进行计算，两种方法都支持在AdaBoost上完成多分类任务，但SAMME.R往往能够得到更好的结果，因此sklearn中的默认值是SAMME.R，因此**sklearn中默认可以输入的base_estimators也需要是能够输出预测概率的弱评估器。实际在预测时，AdaBoost输出的$H(x)$也针对于某一类别的概率**。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDvaJo3jXyYf"
      },
      "source": [
        "需要注意的是，在分类器中，我们虽然被允许选择算法，却不被允许选择算法所使用的损失函数，这是因为SAMME与SAMME.R使用了相同的损失函数：二分类指数损失（Exponential Loss Function）与多分类指数损失（Multi-class Exponential loss function）。\n",
        "\n",
        "**二分类指数损失**——\n",
        "$$L(H(x),y) = e^{-yH^*(x)}$$<br>\n",
        "其中y为真实分类，$H^*(x)$则是从集成算法输出的概率结果$H(x)$转换来的向量。转换规则如下：\n",
        "\n",
        "$$H^*(x)=\n",
        "\\begin{cases}\n",
        "1& if \\ H(x)>0.5 \\\\\n",
        "-1& if\\  H(x) < 0.5\n",
        "\\end{cases}$$\n",
        "\n",
        "在sklearn当中，由于$H(x)$是概率值，因此需要转换为$H^*(x)$，如果在其他实现AdaBoost的算法库中，$H(x)$输出直接为预测类别，则可以不执行转换流程。\n",
        "\n",
        "**根据指数损失的特殊性质，二分类状况下的类别取值只能为-1或1**，因此$y$的取值只能为-1或1。当算法预测正确时，$yH^*(x)$的符号为正，则在函数$e^{-x}$上损失很小。当算法预测错误时，$yH^*(x)$的符号为负，则在函数$e^{-x}$上损失较大。二分类指数损失是AdaBoost最经典的损失函数，它在数学推导上的有效性以及在实践过程中很强的指导性让其沿用至今。\n",
        "\n",
        "![](https://www.tf.uni-kiel.de/matwis/amat/mw1_ge/kap_5/illustr/exponential1com.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nts8DYJdXyYg"
      },
      "source": [
        "**多分类指数损失**——\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L(H(x),y) &=exp \\left( -\\frac{1}{K}\\boldsymbol{y^* · H^*(x)} \\right) \\\\ \n",
        "& = exp \\left( -\\frac{1}{K}(y^{*1}H^{*1}(x)+y^{*2}H^{*2}(x) \\ + \\  ... + y^{*k}H^{*k}(x)) \\right)\n",
        "\\end{aligned}\n",
        "$$<br>\n",
        "其中，$K$为总类别数，如四分类[0,1,2,3]的情况时，$K=4$，$\\boldsymbol{y^*}$与$\\boldsymbol{H^*(x)}$都是根据多分类具体情况、以及集成算法实际输出$H(x)$转化出的向量，其中$y^{*1}$与$H^{*1}(x)$的上标1都表示当前类别。\n",
        "\n",
        "在二分类算法中，算法会直接针对二分类中的**其中一个类别**输出概率，因为在二分类中$P(Y=1) = 1 - P(Y=-1)$，所以只计算出一类的概率即可判断预测的标签。但在多分类算法中，算法必须针对所有可能的取值类型都输出概率，**才能够从中找出最大概率所对应的预测标签**。因此在集成算法中，我们对进行多分类预测时，会得到如下的表格："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Y9huHYXyYg",
        "outputId": "fb7cf0b4-b6cc-4f6c-c865-cd9f56aeda86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.909574</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010638</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031915</td>\n",
              "      <td>0.031915</td>\n",
              "      <td>0.015957</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.131380</td>\n",
              "      <td>0.120038</td>\n",
              "      <td>0.157845</td>\n",
              "      <td>0.134216</td>\n",
              "      <td>0.011342</td>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.163516</td>\n",
              "      <td>0.158790</td>\n",
              "      <td>0.115312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.131380</td>\n",
              "      <td>0.120038</td>\n",
              "      <td>0.157845</td>\n",
              "      <td>0.134216</td>\n",
              "      <td>0.011342</td>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.163516</td>\n",
              "      <td>0.158790</td>\n",
              "      <td>0.115312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.092672</td>\n",
              "      <td>0.099138</td>\n",
              "      <td>0.032328</td>\n",
              "      <td>0.071121</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.370690</td>\n",
              "      <td>0.012931</td>\n",
              "      <td>0.006466</td>\n",
              "      <td>0.002155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.909574</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010638</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031915</td>\n",
              "      <td>0.031915</td>\n",
              "      <td>0.015957</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.909574  0.000000  0.010638  0.000000  0.031915  0.031915  0.015957   \n",
              "1  0.003781  0.131380  0.120038  0.157845  0.134216  0.011342  0.003781   \n",
              "2  0.003781  0.131380  0.120038  0.157845  0.134216  0.011342  0.003781   \n",
              "3  0.000000  0.092672  0.099138  0.032328  0.071121  0.312500  0.370690   \n",
              "4  0.909574  0.000000  0.010638  0.000000  0.031915  0.031915  0.015957   \n",
              "\n",
              "          7         8         9  \n",
              "0  0.000000  0.000000  0.000000  \n",
              "1  0.163516  0.158790  0.115312  \n",
              "2  0.163516  0.158790  0.115312  \n",
              "3  0.012931  0.006466  0.002155  \n",
              "4  0.000000  0.000000  0.000000  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#多分类预测\n",
        "clf = DTC(max_depth=2).fit(X_c,y_c)\n",
        "\n",
        "#多分类预测输出的概率结果，取前5个样本\n",
        "pd.DataFrame(clf.predict_proba(X_c)).iloc[:5,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6SpFLoYXyYg"
      },
      "source": [
        "每一行对应一个样本，每一列则对应该样本的预测标签为某一类别的概率，以上表格就是5个样本在10分类情况下得出的概率表格，**而每一个样本的10个概率中，最大概率所对应的类别就是预测类别**。而这一转换可以由函数argmax完成。argmax会取出最大值所对应的索引，刚好也就是最大概率所对应的预测标签。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmiw6zxcXyYg",
        "outputId": "8779f031-a3cd-4d00-9025-e68c059953d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(pd.DataFrame(clf.predict_proba(X_c)).iloc[0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6-ug7csXyYg",
        "outputId": "9d3496f5-9475-4b9b-8103-5cd7005b682e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(pd.DataFrame(clf.predict_proba(X_c)).iloc[1,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWKhdA1rXyYg",
        "outputId": "b0d8226c-f24e-4a1e-fa82-43e623248b4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(pd.DataFrame(clf.predict_proba(X_c)).iloc[3,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2l3iXAXXyYh"
      },
      "source": [
        "对一棵决策树我们会输出k个概率，对于boosting集成中的每一棵树，在任意样本上都会得到$f^{c=0}(x)$、$f^{c=1}(x)$、$f^{c=2}(x)$……数个不同的结果。在集成算法当中，每个样本在第t次建树过程中，都会生成针对于不同类别的结果：\n",
        "\n",
        "$$H_{t}^0(x_i) = H_{t-1}^0(x_i) + \\phi_tf_t^0(x_i)$$<br>\n",
        "$$H_{t}^1(x_i) = H_{t-1}^1(x_i) + \\phi_tf_t^1(x_i)$$<br>\n",
        "$$H_{t}^2(x_i) = H_{t-1}^2(x_i) + \\phi_tf_t^2(x_i)$$<br>\n",
        "$$……$$\n",
        "$$H_{t}^k(x_i) = H_{t-1}^k(x_i) + \\phi_tf_t^k(x_i)$$<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Up7m3fXyYh"
      },
      "source": [
        "因此，我们可以得到向量$[H^0(x),H^1(x),H^2(x),...,H^k(x)]$，表示当前集成算法计算出的、针对多个类别的概率（也是对全部弱分类器输出的、针对多个类别的概率进行的加权求和）。针对该向量，一定可以得到向量中的一个最大值，该最大值所对应的标签类别就是多分类算法中的预测标签类别。**根据该向量，以及指数损失的特性，我们规定：**\n",
        "\n",
        "$$H^*(x)=\n",
        "\\begin{cases}\n",
        "1& if \\ k = argmaxH(x) \\\\\n",
        "-\\frac{1}{K-1}& if\\ k  \\neq  argmaxH(x)\n",
        "\\end{cases}$$\n",
        "\n",
        "其中，$argmaxH(x)$对应的是预测标签，$k$为所有预选标签类别。因此，假设在4分类情况下，集成算法针对样本$i$的各个分类输出的概率如下所示，则向量$\\boldsymbol{H^*(x)}$的取值如下所示：\n",
        "\n",
        "||$0$|$1$|$2$|$3$|\n",
        "|:-:|:-:|:-:|:-:|:-:|\n",
        "|$H_t^k(x_i)$|$0.1$|$0.2$|$0.2$|$0.5$|\n",
        "|$H^*(x)$|$-\\frac{1}{3}$|$-\\frac{1}{3}$|$-\\frac{1}{3}$|$1$|\n",
        "\n",
        "其中3就是当前集成算法针对样本$i$预测的标签。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GgRjWMnXyYh"
      },
      "source": [
        "另外一方面，$\\boldsymbol{y^*}$一般来说都是真实标签经过上述处理后的结果。同样是4分类器情况下，**假设样本$i$的真实标签为2**，则向量$\\boldsymbol{y^*}$的构成如下所示：\n",
        "\n",
        "||$0$|$1$|$2$|$3$|\n",
        "|:-:|:-:|:-:|:-:|:-:|\n",
        "|$\\boldsymbol{y^*}$|$-\\frac{1}{3}$|$-\\frac{1}{3}$|$1$|$-\\frac{1}{3}$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2vCg4qaXyYh"
      },
      "source": [
        "用公式表示则有：\n",
        "$$y^*=\n",
        "\\begin{cases}\n",
        "1& if \\ k=y_i \\\\\n",
        "-\\frac{1}{K-1}& if\\  k\\neq y_i \n",
        "\\end{cases}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfxwJBZpXyYh"
      },
      "source": [
        "其中$y_i$为样本的真实标签，$k$为所有预选标签类别。不难发现，在此规则下，此时向量$\\boldsymbol{y^*}$以及向量$\\boldsymbol{H^*(x)}$的和永远是0，因为向量内部总是1与(K-1)个$-\\frac{1}{K-1}$相加。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmbyUdAKXyYh",
        "outputId": "b1cdec88-3279-48d9-848d-8ad61487fd9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "K = 4\n",
        "y = [1,(-1/(K-1)),(-1/(K-1)),(-1/(K-1))]\n",
        "int(sum(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8y--xB0XyYi"
      },
      "source": [
        "在多分类算法当中，我们常常求解类似于$\\boldsymbol{y^*}$或$\\boldsymbol{H^*(x)}$的向量，比如在softmax函数中，当预测值或真实值不等于$k$时，我们赋予的向量值为0，而不是$-\\frac{1}{K-1}$。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51eqiKhFXyYi"
      },
      "source": [
        "softmax的一般规则："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-KuE8CKXyYi"
      },
      "source": [
        "||$0$|$1$|$2$|$3$|\n",
        "|:-:|:-:|:-:|:-:|:-:|\n",
        "|$H_t^k(x_i)$|$0.1$|$0.2$|$0.2$|$0.5$|\n",
        "|$H^*(x)$|$0$|$0$|$0$|$1$|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP15YDUSXyYi"
      },
      "source": [
        "同时，当K=2时，多分类指数损失的值与二分类指数损失完全一致："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XXSKW4VXyYi"
      },
      "source": [
        "多分类指数损失：\n",
        "\n",
        "假设K=2，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L = exp \\left( -\\frac{1}{K} \\left( y^{*1}H^{*1}(x)+y^{*2}H^{*2}(x) \\right) \\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "假设预测分类 = 真实分类 = 1，\n",
        "\n",
        "||$1$|$2$|\n",
        "|:-:|:-:|:-:|\n",
        "|$H_t^k(x_i)$|$0.7$|$0.3$|\n",
        "|$H^*(x)$|$1$|$-\\frac{1}{2-1}$|\n",
        "\n",
        "<br>\n",
        "\n",
        "||$1$|$2$|\n",
        "|:-:|:-:|:-:|\n",
        "|$\\boldsymbol{y^*}$|$1$|$-\\frac{1}{2-1}$|\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuEFb8bbXyYi"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "-\\frac{1}{K}&\\left( y^{*1}H^{*1}(x)+y^{*2}H^{*2}(x) \\right)\\\\\n",
        "&= -\\frac{1}{2} \\left( 1 * 1 + \\frac{-1}{2-1} * \\frac{-1}{2-1} \\right)\\\\\n",
        "&= -\\frac{1}{2} \\left( 1 + 1 \\right)\\\\\n",
        "&= -1\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsBcGhJ8XyYi"
      },
      "source": [
        "二分类指数损失，y=1，由于预测正确，所以$H^*(x)$ = 1\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "-y&H^*(x)\\\\\n",
        "& = -(1 * 1)\\\\\n",
        "& = -1\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_A9sWSSXyYj"
      },
      "source": [
        "在实践中，无论是SAMME还是SAMME.R，我们都无法改变使用的损失函数，因此参数中没有为我们提供相应的选择。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgDMmoucXyYj"
      },
      "source": [
        "> - `loss`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB9fR8PDXyYj"
      },
      "source": [
        "看完参数`algorithm`，我们来看参数`loss`。与分类的情况完全相反，在AdaBoost回归当中，我们能够使用的算法是唯一的，即AdaBoost.R2，但是在R2算法下，我们却可以选择三种损失函数，分别是\"linear\"（线性）,\"square\"（平方）,\"exponential\"（指数）。\n",
        "\n",
        "在算法AdaBoost.R2当中，三种损失函数如下定义：\n",
        "\n",
        "首先：\n",
        "$$D = sup|H(x_i) - y_i|, i = 1,2,...,N$$\n",
        "\n",
        "其中$y_i$为真实标签，$H(x_i)$为预测标签，sup表示“取最大值”，但它与直接写作max的函数的区别在于，max中的元素已是固定的数值，而sup中的元素可以是一个表达式、并让该表达式在i的备选值中循环。上述式子表示，取出1~N号样本中**真实值与预测值差距最大的那一组差异**来作为D的值。\n",
        "\n",
        "**R2算法线性损失——**\n",
        "\n",
        "$$L_i = \\frac{|H(x_i) - y_i|}{D}$$\n",
        "\n",
        "**R2算法平方损失——**\n",
        "\n",
        "$$L_i = \\frac{|H(x_i) - y_i|^2}{D^2}$$\n",
        "\n",
        "**R2算法指数损失——**\n",
        "\n",
        "$$L_i = 1 - exp \\left( \\frac{-|H(x_i) - y_i|}{D} \\right)$$\n",
        "\n",
        "不难发现，其实线性损失就是我们常说的MAE的变体，平方损失就是MSE的变体，而指数损失也与分类中的指数损失高度相似。在R2算法当中，这些损失函数特殊的地方在于分母D。由于D是所有样本中真实值与预测值差异最大的那一组差异，因此任意样本的$L_i$在上述线性与平方损失定义下，取值范围都只有[0,1]（当真实值=预测值时，取值为0，当真实值-预测值=D时，取值为1）。\n",
        "\n",
        "![](https://www.tf.uni-kiel.de/matwis/amat/mw1_ge/kap_5/illustr/exponential1com.png)\n",
        "\n",
        "特别的，对于指数损失来说，自变量的部分是在[0,1]中取值，因此$e^{-x}$的在该定义域上的值域也为[0,1]，因此$1-e^{-x}$的值域为[0,1]。事实上，在R2算法的论文当中，就有明确对损失函数的唯一要求：即值域为[0,1]。该规则使得整个AdaBoost算法的求解流程变得顺畅，具体可以在《2 原理进阶：AdaBoost的求解流程》中看到。\n",
        "\n",
        "现在，我们已经了解了AdaBoost的全部参数了。不难发现，在AdaBoost的参数空间中，n_estimators与learning_rate是最为重要的两个参数。当我们在进行超参数调整时，注意对这两个参数的组合进行同时调整即可。\n",
        "\n",
        "|参数|参数含义|\n",
        "|:-:|:-:|\n",
        "|**base_estimator**|弱评估器|\n",
        "|n_estimators|集成算法中弱评估器的数量|\n",
        "|**learning_rate**|迭代中所使用的学习率|\n",
        "|**algorithm**（分类器专属）|用于指定分类ADB中使用的具体实现方法|\n",
        "|**loss**（回归器专属）|用于指定回归ADB中使用的损失函数|\n",
        "|random_state|用于控制每次建树之前随机抽样过程的随机数种子|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXM0X6bRXyYj"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp0j5AqQXyYj"
      },
      "source": [
        "## 2 原理进阶：AdaBoost的求解流程"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpOZ16ZpXyYj"
      },
      "source": [
        "在使用AdaBoost算法时，我们并不需要对AdaBoost的具体求解流程掌握太过于深入。严格来说，只要知道参数的含义，即便我们完全不了解AdaBoost的求解流程，我们也能够自由地调用这个算法。然而，对于参数较少、原理简单的AdaBoost来说或许是okay的，对于后续即将要学习的复杂Boosting算法而言，我们却很难再避开复杂数学推导与原理。即便是为了应对面试中会出现的“你都知道哪些boosting算法？这些算法之间有什么异同？”，我们也必须对Boosting算法的原理有所掌握。\n",
        "\n",
        "对于任意Boosting算法，我们都需要明确以下几点：\n",
        "\n",
        "- 损失函数$L(x,y)$的表达式是什么？损失函数如何影响模型构建？\n",
        "- 弱评估器$f(x)$ 是什么，当下boosting算法使用的具体建树过程是什么？\n",
        "- 综合集成结果$H(x)$是什么？集成算法具体如何输出集成结果？\n",
        "\n",
        "同时，还可能存在其他需要明确的问题，例如：\n",
        "- 是加权求和吗？如果是，加权求和中的权重如何求解？\n",
        "- 训练过程中，拟合的数据$X$与$y$分别是什么？\n",
        "- 模型训练到什么时候停下来最好？\n",
        "\n",
        "同时，别忘记boosting算法的基本规则：\n",
        "\n",
        "---\n",
        "**<font color=\"green\"><center>依据上一个弱评估器$f(x)_{t-1}$的结果，计算损失函数$L(x,y)$，\n",
        "    <br>并使用$L(x,y)$自适应地影响下一个弱评估器$f(x)_t$的构建。<br>集成模型输出的结果，受到整体所有弱评估器$f(x)_0$ ~ $f(x)_T$的影响。</center></font>**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZHIuNhsXyYk"
      },
      "source": [
        "在此基本指导思想下，我们来梳理回归算法的基本流程（考虑到后续Boosting算法也是以回归为主流，因此在这里我们梳理回归算法的基本流程）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVohyV2TXyYk"
      },
      "source": [
        "- **AdaBoost.R2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGPr9sa9XyYk"
      },
      "source": [
        "AdaBoost.R2算法是当前AdaBoost实现流程中使用最多的回归类实践方式，它囊括了对数据进行有放回抽样、按损失函数结果调整样本权重、自动计算弱分类器权重、并输出预测结果等AdaBoost算法经典的全流程。假设现有数据集N，含有样本$M$个，任意样本编号为$i$，同时，弱评估器为决策树$f$，总共学习$T$轮，则AdaBoost.R2的基本流程如下所示："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5W3ddKIXyYk"
      },
      "source": [
        "- 1) 初始化原始数据集的权重$w_i$，其中任意$w_i = \\frac{1}{M}$\n",
        "\n",
        "开始循环，for t in 1,2,...T:\n",
        "\n",
        "> - 2) 在现有数据集$N$中，有放回抽样$M$个样本，构成训练集$N^t$。在每次抽取一个样本时，任意样本被抽中的概率为$P_i^t = \\frac{w_i}{\\sum w_i}$，很显然，**该概率就是当前样本在训练集$N^t$中的权重**。当从初始权重中抽样时，概率$P_i^1 = \\frac{1}{M}$，当后续权重变化时，拥有更大权重的样本被抽中的概率会更大。<br><br>\n",
        "> - 3) 在训练集$N^t$上按照**CART树**规则建立一棵回归树$f^t$，训练时所拟合的标签为样本的**真实标签**$y^t_i$。<br><br>\n",
        "> - 4) 将$N^t$上所有的样本输入$f^t$进行预测，得出预测结果$f^t(x_i)$，其中i = 1,2,...M。<br><br>\n",
        "> - 5) 计算单一样本$i$上的损失函数$L^t_i = L(f^t(x_i),y_i)$，计算过程如下所示：\n",
        ">> 求解$D = sup|f^t(x_i) - y_i|, i = 1,2,...,N$<br><br>\n",
        ">> 选择线性/平方或指数损失函数中的一种计算$L^t_i$<br><br>\n",
        ">> 线性损失：$L_i = \\frac{|f^t(x_i) - y_i|}{D}$<br><br>\n",
        ">> 平方损失：$L_i = \\frac{|f^t(x_i) - y_i|^2}{D^2}$<br><br>\n",
        ">> 指数损失：$L_i = 1 - exp \\left( \\frac{-|f^t(x_i) - y_i|}{D} \\right)$<br><br>\n",
        ">> 根据AdaBoost的要求，所有损失的值域都在[0,1]之间。\n",
        "> - 6) 计算全样本上的加权平均损失$\\bar{L^t} = \\sum_{i=1}^ML_i^tP_i^t$\n",
        ">> 注意此时$P_i^t$就等于样本的权重。由于$P_i^t = \\frac{w_i}{\\sum w_i}$，所以$P_i^t$一定位于[0,1]范围内，并且$\\sum{P_i^t}, i=1,2,...M$一定为1。<br><br>\n",
        ">> **当权重之和为1时，加权平均值一定会小于等于单一数值的最大值（同时大于等于单一数值的最小值），因此加权平均的值域不会超出单一平均数的值域**。由于所有损失的值域都是[0,1]，因此加权平均值$\\bar{L^t}$的值域也是[0,1]。同时，由于损失的最大值为1，而权重$P_i^t$的最大值一定是远远小于1的，因此加权平均值$\\bar{L^t}$的最大值一般也是远远小于1的。<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76hwCrJCXyYk"
      },
      "outputs": [],
      "source": [
        "#例如：\n",
        "l = [10,20,5]\n",
        "P = [0,1,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GHbL9zXyYk"
      },
      "source": [
        "> - 7) 依据加权平均损失$\\bar{L^t}$计算衡量当前集成算法的置信度$\\beta^t$\n",
        ">> $\\beta^t = \\frac{\\bar{L^t}}{1-\\bar{L^t} + \\lambda}$，其中$\\lambda$是为了防止分母为0的常数<br><br>\n",
        ">> 不难发现，当加权平平均损失很高时，$\\beta^t$很大，因此置信度小，当加权平均损失很低时，$\\beta^t$很小，因此置信度大。置信度越大，集成算法当前的预测结果越好。<br><br>\n",
        ">> 已知$\\bar{L^t}$的理论值域是[0,1]，因此$\\beta^t$的理论值域是[0,$+\\infty$]，因此$\\beta_t$的值越接近0越好。<br><br>\n",
        ">>同时，我们还知道$\\bar{L^t}$的实际范围大约都在0.2~0.3之间，因此一般来说$\\beta^t$的实际范围基本都是小于1的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "POM9Xt0vXyYl",
        "outputId": "4fec6013-f439-4bae-ff14-71dfc991d1cc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "M = 1000\n",
        "lambda_ = 1e-6\n",
        "lt = np.sort(np.random.rand(M))\n",
        "beta = lt/(1-lt+lambda_)\n",
        "plt.plot(lt,beta)\n",
        "plt.ylim(0,5)\n",
        "plt.vlines(0.5,0,1,linestyles=\"dotted\",color=\"red\")\n",
        "plt.hlines(1,0,0.5,linestyles=\"dotted\",color=\"red\")\n",
        "plt.xlabel(\"lt\",fontdict={\"fontsize\":14})\n",
        "plt.ylabel(\"beta\",fontdict={\"fontsize\":14});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ34gwvdXyYl"
      },
      "source": [
        "> - 8) 依据置信度评估$\\beta_t$更新样本权重\n",
        ">> $w_i = w_i\\beta^{(1-L_i)}$<br><br>\n",
        ">> 我们可以根据$L_i$的范围[0,1]，以及$\\beta$的计算公式，绘制出横坐标为$L_i$，纵坐标为$\\beta^{(1-L_i)}$的图像。不难发现，**单一样本的损失越大、$\\beta^{(1-L_i)}$也会越大，因此该样本的权重会被更新得越大**。<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "D5wWz86pXyYl",
        "outputId": "51db534a-d6a5-4fca-86bd-f199f063e2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3902868424714614 0.6401144497832878\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#假设1000个样本\n",
        "M = 1000\n",
        "lambda_ = 1e-6\n",
        "\n",
        "#1000个样本的损失位于[0,1]之间\n",
        "l = np.linspace(0,0.8,M)\n",
        "\n",
        "#1000个样本被抽到的概率加和为1\n",
        "p = np.random.dirichlet(np.ones(M),size=1)\n",
        "#p = 1/M\n",
        "\n",
        "#计算加权平均值与beta\n",
        "lbar = (l*p).sum()\n",
        "beta = lbar/(1-lbar+lambda_)\n",
        "\n",
        "l = np.sort(l,axis=0) #从小到大进行排序\n",
        "\n",
        "#按照1000个样本的l对beta**(1-l)进行绘图\n",
        "plt.plot(l,beta**(1-l))\n",
        "plt.xlabel(\"L_i\",fontdict={\"fontsize\":14})\n",
        "plt.ylabel(\"beta^(1-l)\",fontdict={\"fontsize\":14});\n",
        "print(lbar, beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uc255ypXyYl"
      },
      "source": [
        "> - 9) 求解迭代过程中弱分类器$f^t$所需的权重\n",
        ">> $\\phi^t = log(\\frac{1}{\\beta^t})$\\\n",
        ">> 其中log的底数为e或者为2皆可。当$\\beta$值越接近于0，说明损失越小、置信度越高，则$log(\\frac{1}{\\beta^t})$的值越大。所以，损失更小的树对应的权重更大，损失更大的树对应的权重更小。\n",
        "> - 10) 求解出当前迭代$t$下集成算法的输出值：\n",
        ">> $H^t(x_i) = H^{t-1}(x_i) + \\eta \\phi^t f^t(x_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GZwCmv5XyYl"
      },
      "source": [
        "在步骤2~10中循环，直到迭代次数被使用完毕。理想上来说，Adaboost至少应该迭代到$T$次以满足下列条件：\n",
        "\n",
        "$$\\left(\\sum_{t:H^t(x) \\leq y} log\\frac{1}{\\beta^t} \\right)\\ \\  \\geq \\ \\ \\left(\\frac{1}{2}\\sum_{t=1}^T log\\frac{1}{\\beta^t} \\right)$$\n",
        "\n",
        "等同于：\n",
        "$$\\left(\\sum_{t:H^t(x) \\leq y} \\phi^t \\right)\\ \\  \\geq \\ \\ \\left(\\frac{1}{2}\\sum_{t=1}^T \\phi^t \\right)$$\n",
        "\n",
        "并且，最终算法的输出值是上述等式满足“等于”条件时所对应的$H^t(x)$。对于一个正常迭代的AdaBoost来说，每一轮迭代后获得的$H(x_i)$都是累加结果，因此$H(x_i)$之间应该满足以下关系：\n",
        "\n",
        "$$H^0(x_i) < H^1(x_i) <, ... , < H^T(x_i)$$\n",
        "\n",
        "在$H^0(x_i)$到$H^T(x_i)$过程中，必然只有部分$H(x_i)$是小于真实标签$y_i$的，假设有$t$次迭代中$H(x_i)$都小于$y_i$，则理想状况下，前$t$次迭代中权重的累加，应该大于0.5 * 所有$T$次迭代中权重的累加。当两者相等时，t就是最佳迭代次数，而$t$对应的$H^t(x)$也就是最佳预测值。\n",
        "\n",
        "要完全使用公式来证明以上式子非常困难，但我们可以通过一个简单的小实验来验证该公式的合理性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9i_UIZnXyYm"
      },
      "outputs": [],
      "source": [
        "yi = 20\n",
        "lambda_ = 1e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0Ld4jjsXyYm"
      },
      "outputs": [],
      "source": [
        "Hx = np.linspace(1,25,1000,endpoint=False) #逐渐增大的Hx\n",
        "\n",
        "#原则上应该使用fx计算损失，并且在迭代过程中逐渐计算出权重\n",
        "#但由于计算过程略为复杂，因此我们在这里简化，直接使用Hx计算损失、beta和权重\n",
        "#这种方法得出的曲线不是最严谨的，但其趋势与使用fx严谨计算的曲线趋势一致\n",
        "#因为原则上来说，只要一个样本被AdaBoost分类正确，这个样本上的损失应该也是越来越小的\n",
        "D = np.max(abs(Hx - yi))\n",
        "L = abs(Hx - yi)/D\n",
        "beta = L/(1-L+lambda_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLi9Hs2TXyYm"
      },
      "outputs": [],
      "source": [
        "part1 = 0 #用来计算每一轮迭代后的累加值\n",
        "part1_ = [] #用来保存每一轮迭代后的累加值\n",
        "part2 = 0\n",
        "part2_ = []\n",
        "for t, beta_t in enumerate(beta):\n",
        "    phi = np.log(1/beta_t)\n",
        "    #如果Hx小于真实标签yi，则取倒数取对数后放入part1内\n",
        "    if Hx[t] <= yi:\n",
        "        part1 += phi\n",
        "    part1_.append(part1)\n",
        "    #所有beta取倒数取对数 * 0.5后都放入part2内\n",
        "    part2 += 0.5*phi\n",
        "    part2_.append(part2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGT2cg6yXyYm",
        "outputId": "9fce0b9b-640f-4248-cdfd-8b5da5395ddb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(part1_)),part1_,c=\"red\",label = \"h < y t\")\n",
        "plt.plot(range(1000),part2_,c=\"blue\",label = \"all t\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUwbEdSdXyYm",
        "outputId": "c4b34480-1369-4dfd-e61d-b4701449ae25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19.96"
            ]
          },
          "execution_count": 299,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#最佳输出值\n",
        "Hx[790]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZF6gkCCXyYm"
      },
      "source": [
        "在课程当中我们专注于AdaBoost回归的讲解，如果你对AdaBoost分类感兴趣，可以在任意材料上找到AdaBoost经典分类方法的数学推导，也可以找到包含了SAMME与SAMME.R方法的数学推导，值得注意的是，在AdaBoost回归方法当中，损失函数并没有明显的被“最小化”的过程，我们借助损失函数来自然地调整数据的权重，从而在迭代中不断减小整体损失，但在AdaBoost分类方法当中，有针对损失函数求解一阶导数、并让一阶导数为0的过程，该过程影响AdaBoost分类过程中求解弱分类器权重的内容，同时也影响AdaBoost分类过程中对于样本权重的迭代更新。感兴趣的小伙伴可以参阅论文《Multi-class AdaBoost》以及sklearn源码(https://github.com/scikit-learn/scikit-learn/blob/0d378913b/sklearn/ensemble/_weight_boosting.py#L913)。\n",
        "\n",
        "现在，我们已经完整讲解了AdaBoost的回归过程，该过程中的关键步骤可以被推广到任意Boosting算法，相信了解了这个过程之后，在学习后续Boosting算法时，我们将轻松很多。之后，我们将基于该流程讲解后续Boosting算法的原理。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Lesson 11 AdaBoost.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}